{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGg7VDvDcHRM",
        "outputId": "4571f91d-a02e-4e38-f5c7-ecaef93f63c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OFres0fbxBB",
        "outputId": "cf85120f-5731-4fdd-dcdb-f503d990714e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge_score) (3.2.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge_score) (0.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.19.5)\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.0.4\n",
            "2.7.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import collections\n",
        "import scipy.spatial.distance as ds\n",
        "import pickle\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "from string import digits\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as f\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "!pip install rouge_score\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "import matplotlib.ticker as ticker\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from __future__ import absolute_import, division, print_function\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import io\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKj3hIMrcPVh",
        "outputId": "5c980d71-8cc7-4271-b601-40efeb1ecf62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dakshina_dataset_v1.0/bn/\n",
            "dakshina_dataset_v1.0/bn/lexicons/\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/romanized/\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/\n",
            "dakshina_dataset_v1.0/gu/lexicons/\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/romanized/\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/\n",
            "dakshina_dataset_v1.0/hi/lexicons/\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/romanized/\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/\n",
            "dakshina_dataset_v1.0/kn/lexicons/\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/kn/romanized/\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/\n",
            "dakshina_dataset_v1.0/ml/lexicons/\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/romanized/\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/\n",
            "dakshina_dataset_v1.0/mr/lexicons/\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/romanized/\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/\n",
            "dakshina_dataset_v1.0/pa/lexicons/\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/pa/romanized/\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/\n",
            "dakshina_dataset_v1.0/sd/lexicons/\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/sd/romanized/\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/\n",
            "dakshina_dataset_v1.0/si/lexicons/\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/romanized/\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/\n",
            "dakshina_dataset_v1.0/ta/lexicons/\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/romanized/\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/\n",
            "dakshina_dataset_v1.0/te/lexicons/\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/romanized/\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ur/\n",
            "dakshina_dataset_v1.0/ur/lexicons/\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/romanized/\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/README.md\n"
          ]
        }
      ],
      "source": [
        "# !tar -xvf  \"/content/drive/My Drive/Adv_NLP_Project/dakshina_dataset_v1.0.tar\" -C \"/content/sample_data/\" \n",
        "# !tar -xvf  \"/content/drive/MyDrive/NLP/Project/Dataset/dakshina_dataset_v1.0.tar\" -C \"/content/sample_data/\"\n",
        "!tar -xvf  \"/content/drive/MyDrive/NLP/dakshina_dataset_v1.0.tar\" -C \"/content/sample_data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_184dB5cch43"
      },
      "outputs": [],
      "source": [
        "l_train = []\n",
        "l_test = []\n",
        "\n",
        "f1=\"/content/sample_data/dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\"\n",
        "curr_file1 = open(f1, \"r\")\n",
        "for line in curr_file1:\n",
        "  temp = []\n",
        "  temp.append(line)\n",
        "  l_train.append(temp)\n",
        "curr_file1.close()\n",
        "\n",
        "f2=\"/content/sample_data/dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\"\n",
        "curr_file2 = open(f2, \"r\")\n",
        "i=0\n",
        "for line in curr_file2:\n",
        "  l_train[i].append(line)\n",
        "  i+=1\n",
        "curr_file2.close()\n",
        "\n",
        "f3=\"/content/sample_data/dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\"\n",
        "curr_file3 = open(f3, \"r\")\n",
        "for line in curr_file3:\n",
        "  temp = []\n",
        "  temp.append(line)\n",
        "  l_test.append(temp)\n",
        "curr_file1.close()\n",
        "\n",
        "f4=\"/content/sample_data/dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\"\n",
        "curr_file4 = open(f4, \"r\")\n",
        "i=0\n",
        "for line in curr_file4:\n",
        "  l_test[i].append(line)\n",
        "  i+=1\n",
        "curr_file2.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXVMQHReiAm8",
        "outputId": "db953009-1db6-44e8-bc0e-15f112ecc5d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['इसके आने से पूर्व ही लोग घरों की सफाई का कार्य शुरू कर देते हैं।\\n', 'iske aane se purva hi log gharon ki safai ka karya shuru kar dete hain.\\n'], ['विलुप्ति की कगार पर गुणकारी तीखुर\\n', 'vilupti ki kagaar par gunkaari tikhur\\n'], ['माइकल कामेन (द वॉल के वाद्यमय हिस्सों के लिए एक योगदानकर्ता) ने दोनों के बीच मध्यस्थता की और उस भूमिका को निभाया जिस पर पारंपरिक रूप से रिचर्ड राइट ने कब्जा किया हुआ था जो अब अनुपस्थित थे।\\n', 'Michael kamen (the wall ke vadyamaya hisso ke liye ek yogdanakarta) ne dono ke bich madhyasthata ki aur us bhumika ko nibhaya jis par paramparik rup se richard wright ne kabja kiya hua tha jo ab anupasthit the.\\n'], ['शाहनामा नौरोज़ के त्यौहार को महान जमशेद के शासनकाल से जोड़ता है।\\n', 'shahnama noroj ke tyohaar ko mahaan Jamshed ke shashankaal se jodta hai.\\n'], ['मेहरोत्रा, डॉ॰ एन.\\n', 'Mehrotra, Dr॰ N.\\n'], ['दुर्गा भाभी\\n', 'durga bhabhi\\n'], ['स्वामी बृहस्पति\\n', 'swami brhaspati\\n'], ['बॉलीवुड, हिन्दी चलचित्र उद्योग भी मुंबई में ही स्थित है।\\n', 'bollywood, hindi chalchitra udyog bhi mumbai me hi sthit hai.\\n'], ['क्योंकि ऐसे शब्द भाषा के मूल शब्द होते हैं।\\n', 'kyonki aise sabda bhasa ke mula sabda hote haim.\\n'], ['शार्ंगदेव (1210–1247) भारत के संगीतशास्त्री थे जिन्होने संगीतरत्नाकर नामक महत्वपूर्ण ग्रंथ की रचना की।\\n', 'Sharangdev (1210-1247) Bharat ke Sangeetshastri the jinhone Sangeetratankar Naamak mahatwpurn granth ki rachna ki.\\n']]\n",
            "[['कुंभ राशि में जन्मे लोग संभावनाओं से भरी एक जगह के रूप में दुनिया को देखते हैं।\\n', 'Kumbh rashi men janme log sambhavnaon se bhari ek jagah ke roop men duniya ko dekhte hain.\\n'], ['इसका उलट भी सत्य है।\\n', 'Iska ulat bhi satya hai.\\n'], ['कुछ देवता जो मुख्यत: नगर देवता थे, अपने संप्रदायवालों के चलते गुण और कार्य में विशिष्ट समझे गए और उनकी महत्ता सर्वोपरि समझी गई।\\n', 'kuch devta jo mukhyatah nagar devta the, apne sampradayvalon ke chalte gun aur karya mein vishisht samjhe gaye aur unki mahatta sarvopati samjhi gayi.\\n'], ['तेल के उत्पादन में संसार में रोमानिया का छठा स्थान है।\\n', 'Tel ke utpadan men sansar men Romania ka chhath sthan hai.\\n'], ['बनारसी लाल से मिलकर पुलिस ने सारा भेद प्राप्त कर लिया।\\n', 'Banarasi Lal se milkar police ne sara bhed praapt kar liya.\\n'], ['अन्य प्रकार की घटनाओं के प्रति बीमा के समान ही, बचाव निधि द्वारा अन्य निवेशक को मुद्रा निवेश के लिए भुगतान करना पड़ता है।\\n', 'Anya prakar ki ghatanaon ke prati beema ke samaan hi, bachav nidhi dwara anya niveshak ko mudra nivesh ke liye bhugtan karana padta hai.\\n'], ['पंडित शिवकुमार शर्मा\\n', 'Pandit Shivkumar Sharma\\n'], ['उन्हें एक समूह या एक समुदाय अच्छा लगता है, तो वे लगातार दूसरे लोगों से घिरे रहने का प्रयास करते हैं।\\n', 'Unhe ek samuh ya ek samuday achha lagta hai, to ve lagatar dusre login se ghire rahane ka prayas karte hain.\\n'], ['बीबीसी हिन्दी संवाददाता द्वारा वेंकटरामन रामकृष्णन का साक्षात्कार।\\n', 'beebeesee hindi sanwaddaata dwara Venkatraman Ramkrishnan ka saakshatkar.\\n'], ['हाँ, खोज का सिलसिला न रुके,\\n', 'haan, khoj kaa silsilaa na ruke,\\n']]\n"
          ]
        }
      ],
      "source": [
        "print(l_train[:10])\n",
        "print(l_test[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "HZGXH0EKlnl0",
        "outputId": "2b7199de-f68e-4358-96f1-182beda8771a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>roman</th>\n",
              "      <th>native</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>इसके आने से पूर्व ही लोग घरों की सफाई का कार्य...</td>\n",
              "      <td>iske aane se purva hi log gharon ki safai ka k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>विलुप्ति की कगार पर गुणकारी तीखुर\\n</td>\n",
              "      <td>vilupti ki kagaar par gunkaari tikhur\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>माइकल कामेन (द वॉल के वाद्यमय हिस्सों के लिए ए...</td>\n",
              "      <td>Michael kamen (the wall ke vadyamaya hisso ke ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>शाहनामा नौरोज़ के त्यौहार को महान जमशेद के शास...</td>\n",
              "      <td>shahnama noroj ke tyohaar ko mahaan Jamshed ke...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>मेहरोत्रा, डॉ॰ एन.\\n</td>\n",
              "      <td>Mehrotra, Dr॰ N.\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>किन्तु लगता है कि यह मलयालम युक्तिभाषा के बाद ...</td>\n",
              "      <td>kintu lagata hai ki yaha malayalama yuktibhasa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>अक्षांश\\n</td>\n",
              "      <td>akshansh\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4997</th>\n",
              "      <td>वह लापरवाही से एनामारिया, डेवी जोन्स और अन्य स...</td>\n",
              "      <td>vah laparvahi se enamaria, devi jons aur anya ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4998</th>\n",
              "      <td>\"मैं सोचती हूँ कि असली वजह यह थी कि निर्देशक ज...</td>\n",
              "      <td>\"main sochti hun ki asli vajah yah thi ki nird...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999</th>\n",
              "      <td>पटकाई भारत के पूर्वोत्तर में बर्मा के साथ लगी ...</td>\n",
              "      <td>Ptakai Bharat ke purvottar men Burma ke saath ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  roman                                             native\n",
              "0     इसके आने से पूर्व ही लोग घरों की सफाई का कार्य...  iske aane se purva hi log gharon ki safai ka k...\n",
              "1                   विलुप्ति की कगार पर गुणकारी तीखुर\\n            vilupti ki kagaar par gunkaari tikhur\\n\n",
              "2     माइकल कामेन (द वॉल के वाद्यमय हिस्सों के लिए ए...  Michael kamen (the wall ke vadyamaya hisso ke ...\n",
              "3     शाहनामा नौरोज़ के त्यौहार को महान जमशेद के शास...  shahnama noroj ke tyohaar ko mahaan Jamshed ke...\n",
              "4                                  मेहरोत्रा, डॉ॰ एन.\\n                                 Mehrotra, Dr॰ N.\\n\n",
              "...                                                 ...                                                ...\n",
              "4995  किन्तु लगता है कि यह मलयालम युक्तिभाषा के बाद ...  kintu lagata hai ki yaha malayalama yuktibhasa...\n",
              "4996                                          अक्षांश\\n                                         akshansh\\n\n",
              "4997  वह लापरवाही से एनामारिया, डेवी जोन्स और अन्य स...  vah laparvahi se enamaria, devi jons aur anya ...\n",
              "4998  \"मैं सोचती हूँ कि असली वजह यह थी कि निर्देशक ज...  \"main sochti hun ki asli vajah yah thi ki nird...\n",
              "4999  पटकाई भारत के पूर्वोत्तर में बर्मा के साथ लगी ...  Ptakai Bharat ke purvottar men Burma ke saath ...\n",
              "\n",
              "[5000 rows x 2 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create the pandas DataFrame\n",
        "df = pd.DataFrame(l_train, columns = ['native', 'roman'])\n",
        "  \n",
        "# print dataframe.\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fnx-f1THTmV8",
        "outputId": "68d17bec-fbfc-4fa9-e608-8bf662b7e328"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'<start>': 0, '<end>': 1, 'ऀ': 1, 'ँ': 2, 'ं': 3, 'ः': 4, 'ऄ': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ऌ': 13, 'ऍ': 14, 'ऎ': 15, 'ए': 16, 'ऐ': 17, 'ऑ': 18, 'ऒ': 19, 'ओ': 20, 'औ': 21, 'क': 22, 'ख': 23, 'ग': 24, 'घ': 25, 'ङ': 26, 'च': 27, 'छ': 28, 'ज': 29, 'झ': 30, 'ञ': 31, 'ट': 32, 'ठ': 33, 'ड': 34, 'ढ': 35, 'ण': 36, 'त': 37, 'थ': 38, 'द': 39, 'ध': 40, 'न': 41, 'ऩ': 42, 'प': 43, 'फ': 44, 'ब': 45, 'भ': 46, 'म': 47, 'य': 48, 'र': 49, 'ऱ': 50, 'ल': 51, 'ळ': 52, 'ऴ': 53, 'व': 54, 'श': 55, 'ष': 56, 'स': 57, 'ह': 58, 'ऺ': 59, 'ऻ': 60, '़': 61, 'ऽ': 62, 'ा': 63, 'ि': 64, 'ी': 65, 'ु': 66, 'ू': 67, 'ृ': 68, 'ॄ': 69, 'ॅ': 70, 'ॆ': 71, 'े': 72, 'ै': 73, 'ॉ': 74, 'ॊ': 75, 'ो': 76, 'ौ': 77, '्': 78, 'ॎ': 79, 'ॏ': 80, 'ॐ': 81, '॑': 82, '॒': 83, '॓': 84, '॔': 85, 'ॕ': 86, 'ॖ': 87, 'ॗ': 88, 'क़': 89, 'ख़': 90, 'ग़': 91, 'ज़': 92, 'ड़': 93, 'ढ़': 94, 'फ़': 95, 'य़': 96, 'ॠ': 97, 'ॡ': 98, 'ॢ': 99, 'ॣ': 100, '।': 101, '॥': 102, '०': 103, '१': 104, '२': 105, '३': 106, '४': 107, '५': 108, '६': 109, '७': 110, '८': 111, '९': 112, '॰': 113, 'ॱ': 114, 'ॲ': 115, 'ॳ': 116, 'ॴ': 117, 'ॵ': 118, 'ॶ': 119, 'ॷ': 120, 'ॸ': 121, 'ॹ': 122, 'ॺ': 123, 'ॻ': 124, 'ॼ': 125, 'ॽ': 126, 'ॾ': 127, 'ॿ': 128}\n"
          ]
        }
      ],
      "source": [
        "letters_native = [chr(letter) for letter in range(2304, 2432)]\n",
        "alphabet_size_native = len(letters_native)\n",
        "\n",
        "letter2ind_native = {'<start>': 0,'<end>' : 1}\n",
        "for index, letter in enumerate(letters_native):\n",
        "    letter2ind_native[letter] = index+1\n",
        "\n",
        "print(letter2ind_native)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6AKNF5-8XznQ"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence_roman(w):\n",
        "    w = w.lower().strip()\n",
        "    ww = ''\n",
        "    # w = (re.sub(r'[-]', ' ', data[i])).lower().strip()\n",
        "    # w = (re.sub(r'[^\\w\\s]', '', data[i])).lower().strip()\n",
        "    ww += ''.join(ch for ch in w if ch not in string.punctuation)\n",
        "    # Remove all numbers from text\n",
        "    remove_digits = str.maketrans('', '', digits)\n",
        "    ww = ww.translate(remove_digits)\n",
        "    ww = ww.rstrip().strip()\n",
        "    ww = '@' + ww + '#'\n",
        "    return ww\n",
        "\n",
        "def preprocess_sentence_native(w):\n",
        "    w = w.lower()\n",
        "    cleaned_line = ''\n",
        "    cleaned_line += ''.join(ch for ch in w if ch not in string.punctuation)\n",
        "    # Remove all numbers from text\n",
        "    remove_digits = str.maketrans('', '', digits)\n",
        "    cleaned_line = cleaned_line.translate(remove_digits)\n",
        "    cleaned_line = re.sub(\"[२३०८१५७९४६।–]\", \"\", cleaned_line)\n",
        "    cleaned_line = cleaned_line.rstrip().strip()\n",
        "    cleaned_line = '@' + cleaned_line + '#'\n",
        "    return cleaned_line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Cg8cEv-JVJ0C"
      },
      "outputs": [],
      "source": [
        "def create_dataset(l):\n",
        "\n",
        "  native_words, native_sents = [], []\n",
        "  roman_words, roman_sents = [], []\n",
        "\n",
        "  for i in range(len(df)):\n",
        "    l[i][0] = preprocess_sentence_native(l[i][0]) # clean native words.\n",
        "    wordlist1 = l[i][0].split()\n",
        "\n",
        "    l[i][1] = preprocess_sentence_roman(l[i][1]) # clean roman sentences.\n",
        "    wordlist2 = l[i][1].split() # clean roman words.\n",
        "\n",
        "    if len(wordlist1) != len(wordlist2):\n",
        "      print('Skipping: ',len(wordlist1),len(wordlist2), l[i][0], ' - ', l[i][1])\n",
        "      continue\n",
        "\n",
        "    native_words.extend(wordlist1)\n",
        "    roman_words.extend(wordlist2)\n",
        "\n",
        "    native_sents.append(wordlist1)\n",
        "    roman_sents.append(wordlist2)\n",
        "    \n",
        "  return [roman_words, native_words], [roman_sents, native_sents]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjE791HMqico",
        "outputId": "eb91edc2-13e8-4ff3-e69a-bd59427229d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping:  28 27 @तांत्रिक साधक पशु वीर दिव्य भावों के द्वारा महाशक्ति की अर्चना करता हुआ सकल ब्रह्म के शक्तिस्वरूप को अनादि चेतन और आनंदरूप समझकर आत्मविवेक की उपलब्धि करता है#  -  @tantrik sadhak pashu veer divya bhawon ke dwara mahashakti ki archana karata hua sakal bramh ke shaktiswaroop ko chetan aur aanandrup samajkar aatmvivek ki uplabdhi karta hai#\n",
            "Skipping:  22 21 @अतीत में भौतिक कागज का विनिमय होता था और इसमें एक चॅक के समाशोधन में पांच व्यावसायिक दिन तक लग जाते थे#  -  @ateet men bhautik kagaj ka winimay hota tha aur ismeanek chak ke samashodhan men panch vyavasayik din tak lag jaate the#\n",
            "Skipping:  16 15 @डॉ॰ वी पी मिरांशी के अनुसार वाकाटक वंश का सर्वश्रेष्ठ राजा विन्ध्यशक्ति का पुत्र प्रवरसेन था#  -  @dr vp  miranshi ke anusar vakatak vansh ka sarvashrestha raja vindhyasakti ka putra pravarasena tha#\n",
            "Skipping:  16 15 @वही परमात्मा सर्जन की इच्छा से अव्यक्त में प्रविष्ट होकर महत् तत्व की रचना करता है#  -  @vahi parmatma sarjan ki ichcha se avyakt mein pravisht hokar mahattva ki rachna karta hai#\n",
            "Skipping:  19 23 @भारतीय निर्वाचन आयोग के जालस्थलपर सांगली लोक सभा निर्वाचन क्षेत्रमें  से राजनैतिक दलोंके चुनाव परिणामोंका ब्यौरा अंग्रेजी भाषा में#  -  @bhartiya nirvachan aayog ke jaalsthal par sangli lok sabha nirvachan kshetr mein  se rajnaitik dalon ke chunav parinamon ka byaura angrezi bhasha mein#\n",
            "Skipping:  23 22 @गिल्मर ने भी अपनी झुंझलाहट व्यक्त की यह शिकायत करते हुए कि राइट के दिशा निर्देशन का अभाव हमें पागल कर रहा था#  -  @gilmar ne apni jhunjhulahat vyakt ki yah shikayat karte huye ki right ke disha nirdeshan ka abhav hamen pagal kar raha tha#\n",
            "Skipping:  10 11 @लडकिया और लड़को अपना सारा विकास  वर्षों मे होजाता हे#  -  @ladkiya aur ladko apna sara vikaas  varshon me ho jata hai#\n",
            "Skipping:  37 36 @में एक प्रचार विज्ञापन चलाया गया जिसमें अमेरिकन एक्सप्रेस कार्ड से की गई प्रत्येक खरीददारी पर अमेरिकन एक्सप्रेस द्वारा स्टेट्यू ऑफ़ लिबर्टी के नवीनीकरण के लिए एक पेनी का यो योगदान किए जाने की बात कही गई#  -  @men ek prachar vigyapan chalaya gaya jismen american express card se kigai pratyek kharidari par  american express dwara statute of liberty ke navinikaran ke liye ek penny ka yo yogdan kiye jane ki baat kahi gai#\n",
            "Skipping:  19 20 @पूरे भारवर्ष में हिमालय की पर्वतश्रेणी तथा कर्नाटक आंध्र प्रदेश राजस्थान गोवा इन प्रदेशमें पवित्र उपवन पाए जाते है#  -  @pure bharatvarsh men himalay ki parvatshreni tatha karnataka andhra pradesh rajasthan goa in pradesh men pavitra upvan paye jate hai#\n",
            "Skipping:  45 46 @न तो वाटर्स और ना मेसन संगीत पढ़ सकते थे और अपनी खुद की अंकन प्रणाली का आविष्कार करते हुए दोनों ने एल्बम के शीर्षक ट्रैक अ सॉसरफुल ऑफ़ सीक्रेट्स निर्मित किया जिस पर बाद में गिल्मर ने टिप्पणी की  एक वास्तुकला चित्रण के समान#  -  @na to waters na to mason sangit padh sakte the aur apni khud ki ankan pranali ka avishkar karte huye donon ne album ke shirshak track a saucerful of secrets nirtmit kiya jis par baad men gilmer ne tippani ki … ek vaastukala chitran ke samaan#\n",
            "Skipping:  27 26 @मोटे तौर पर भारत मे अहीर चरण गद्दी गौढ़ा गुर्जरघोष या घोसी घासी गोवारी गुज्जर गुर्जर ईडुयान कावुन्दन आदि गोपालकों के रूप मेन वर्गीकृत किए गए है#  -  @mote taur par bharat me ahir charan gaddi gaudha gurjarghosh ya ghosi ghasi govari gujjar iduyan kavundan aadi gopalkon ke roop men vargikrut kiye gaye hai#\n",
            "Skipping:  16 17 @ख सन्‌  में विद्रोही होने के अपराध में पटियाला जेल में साढे तीन वर्ष की कैद#  -  @b san १९२६ mein vidrohi hone ke apradh mein patiala jail mein sadhe teen varsh ki qaid#\n",
            "Skipping:  12 11 @मेसन गिल्मर और राइट मंच पर गए जब अंतिम बार रोजर वाटर्स#  -  @mesan gilmar aur right manch par jab antim baar rojer watars#\n",
            "Skipping:  9 10 @इसका उपयोग विस्फोटक तथा हिमकारी मिश्रण बनाने में कियाजाता#  -  @iska upyog visfotak tatha himkaari mishran banane mein kiya jaata#\n",
            "Skipping:  10 9 @वाणी शर्मा — सारिका शंकर सुराना समर की छोटी बहन#  -  @vani sharma  sarika shankar khurana samar ki chhoti bahan#\n",
            "Skipping:  8 7 @ब्रिटिश कंपनी बीएनएससीसुरे उपग्रह प्रौद्योगिकी लिमिटेड — यूकेडीएमसी#  -  @british company bnscsure upgrah praudyogiki limited  ukdmc#\n",
            "Skipping:  9 8 @याचित शर्मा — कबीर कार्तिक और शिखा का बेटा#  -  @yachit sharma  kabir kartik aur shikha ka beta#\n",
            "Skipping:  39 37 @दूसरी तरफ सबसे बड़े निवेश प्रबंधकों में से कुछ प्रबंधक जैसे ब्लैकरॉक blackrock और वैनगार्ड vanguard प्रबंधन टीमों को प्रभावित करने के लिए इंसेंटिव प्रोत्साहन को कम करके बस हरेक कंपनी पर स्वामित्व स्थापित करने की वकालत करते हैं#  -  @dusri taraf sabse bade nivesh prabandhakon men se kuch prabandhak jaise blackrock aur vanguard prabandhan teamon ko prabhavit karne ke liye incentive protsahan ko kam karke bas harek company par swamitwa sthapit karne ki vakalat karte hain#\n",
            "Skipping:  17 18 @इस्पातनिर्माण के समय निकलीं गैसों का उपयोग उर्जा के स्रोत के रूप में किया जा सकता है#  -  @ispaat nirmaan ke samay nikli gaison ka upyog urja ke srot ke roop mein kiya ja sakta hai#\n",
            "Skipping:  23 22 @द वॉल टूर के दौरान एक फीट  मी॰ ऊंची दीवार कार्डबोर्ड ईंटों से बनी बैंड और दर्शकों के बीच में निर्मित की गई#  -  @the wall tour ke dauran ek  feet  m unchi divarcardboard into se bani band aur darshako ke bich me nirmit ki gai#\n",
            "Skipping:  8 9 @दक्षिण अफ्रीकी महिला क्रिकेट टीम का न्यूज़ीलैंड दौरा#  -  @dakshin afriki mahila kriket teem ka new zealand dauraa#\n",
            "Skipping:  10 11 @वेस्टइंडीज ए ने टॉस जीता और मैदान में चुने गए#  -  @west indies a ne toss jita aur maidaan mein chune gaye#\n",
            "Skipping:  25 26 @ओवरव्हेल्मिंग पोस्टऑटो प्लीहा की शल्यक्रिया का संक्रमण opsi जो विवृत जीवों द्वारा प्रेरित कार्यात्मक प्लीहाभाव के कारण होता है जैसे स्ट्रेप्टोकोकस न्युमोनिया तथा हिमोफिलस इन्फ्लुएंजा#  -  @overweighting post auto plihaa ki shalyakriya ka sankraman opsi jo vivrit jivon dwara prerit kaaryaatmak plihaabhaav ke kaaran hota hai jaise streptococcus pneumoniae tatha haemophilus influenza#\n",
            "Skipping:  56 55 @यह ऐसी जाँच नहीं जो इस बात की पुष्टि करे कि कुत्ता में बीमारी है या नहीं है और न ही यह निश्चित तौर पर इस बीमारी की भविष्यवाणी करता है यहां ‍तक कि अगर कोई कुत्ता जेडीसी jdc वाहक है तो उसके शावक इस बीमारी से पीडि़त नहीं होंगे  इसकी भी गारंटी नहीं देता है#  -  @yah aisi janch nahi jo is baat ki prushti kare ki kutta men bimari hai ya nahin hai aur n hi yah nischit taur par is bimari ki bhavishyavani karta hai yahan tak ki agar koi kutta jdc vahak hai to uske shavak is bimari se pidit nahi honge  iski bhi gaurantee nahi deta hai#\n",
            "Skipping:  82 81 @राज्य के संविधान संयुक्त राज्य के संविधान के तहत दिए जाने वाले अधिकारों से परे और इनसे बढ़कर अधिकार प्रदान कर सकते हैं और साथ ही कराधान के संप्रभु अधिकार और सैन्य सेवा सहित अपने दायित्वों को भी अध्यारोपित कर सकते हैं हर राज्य कम से कम एक सैन्य बल रखता है जो राष्ट्रीय सैन्य स्थानान्तरण सेवा राज्य के नेशनल गार्ड के अधीन होता है जबकि कुछ अन्य राज्य एक दूसरा सैन्य बल रखते हैं जो राष्ट्रीयकरण के अधीन नहीं होता है#  -  @rajya ke sanvidhan sayukt rajya ke sanvidhan ke tahat diye jane wale adhikaron se pare aur inse badhkar adhikar pradhan kar sakte hain aur sath hi karadhan ke samprabhu aur sainya seva sahit apne dayitwon ko bhi adhyaropit kar sakte hain  har rajya kam se kam ek sainya bal rakhata hai jo rashtriy sainya sthananataran seva rajya ke national gard ke adhin hota hai jabki kuchh anya rajya ek dusra saiya bal rakhte hain jo rashtriyakaran ke adhin nahin hota hai#\n",
            "Skipping:  39 38 @लापुटा के उड़ान द्वीप के खिलाफ लिंडालीनो के सतही शहर के विद्रोह को बताने वाला भाग iii का छोटा एपिसोड पांच पैराग्राफ से युक्त ड्रेपियर्स लेटर्स के लिए स्पष्ट रूप से अन्योक्तिपरक था जिस पर स्विफ्ट को गर्व था#  -  @laputa ke udaan dwip ke khilap lidalino ke satahi shahar ke vidroh ko batane wala bhag  ka chhota episod panch pairagraf se yukt drepiyars letars ke liye spasht roop se anyoktiparak tha jis par swift ko garv tha#\n",
            "Skipping:  41 42 @इस उपवन की विशेषता यह है की यह उपवन जिस गाँवके करीब होता ही उस गाँवके लोग उसे संभालते है  शासकीय व्यवस्था का इस प्रक्रियामें सहभाग नहीं होता यह उपवन किसी देवताका निवासस्थान मानाजाता है और उसकी पवित्रता संभाली जाती है#  -  @is upavan ki visheshta yeh hai ki yeh upavan jis ganvke karib hota hi us ganvke log use sambhalte hai  shaskiy vyavastha ka is prakriyame sehbhag nahi hotayeh upavan kisi devta ka nivas sthana manajata hai aur uski pavitrta sambhali jati hai#\n",
            "Skipping:  15 14 @घोड़े की सवारी के दौरान उसके मन में इच्छा जगी काश यह घोड़ा मेरा होता#  -  @ghode sawaari ke dauran uske man men icchha jagi kash yah ghoda mera hota#\n",
            "Skipping:  23 22 @गाँव के अधिकतर लोग खाड़ी देशों में काम करते हैं तथा सरकारी नोकारियों में भी हैं जैसे शिक्षा सेना पुलिस और सिविल सर्विसेस#  -  @gaon ke adhiktar log khadi men kaam karte hain tatha sarkari nokariyon men bhi hain jaise shiksha sena police aur civil services#\n",
            "Skipping:  28 27 @टी वी श्रृंखला द रिटर्न ऑफ़ द एंटिलोप ग्रानाडा टेलिविज़न लिलिपुट के तीन नाविकों के रोमांच पर केन्द्रित है जिनका जहाज इंग्लैण्ड में तूफ़ान में डूब गया था#  -  @tv shrinkhla the return of the antilop granada television liliput ke teen navikon ke romanch par kendrit hai jinka jahaaj england men toofaan men dub gaya tha#\n",
            "Skipping:  28 29 @केफ्रांसीसी क्रांति ने सदियों पूर्व कुछ प्रश्नोँ और जोरदार राजनीतिक और सामाजिक बहस के बाद आम जनता द्वारा स्वीकारी गई मान्यताओं और संस्थानों को जड़ से उखाड़ दिया#  -  @ke fransisi kranti ne sadiyon poor kuchh prashon aur jordar rajnitik aur samajik bahas ke baad aam janta dwoara sweekari gai manytaaon aur sansthanon ko jad se ukhad diya#\n",
            "Skipping:  160 159 @जेरूशलम की घेराबंदी siege of jerusalem जेरूसलम की घेराबंदी एक सैन्य संघर्ष का हिसा थी जो वीजान्टिन साम्राज्य और रशीदुन खिलाफ के बीच  ईस्वी में हुई थी यह जब शुरू हुई जब रशीदुन सेना की कमान अबू उबैदाह के तहत नवंबर  ईस्वी में जेरूशलम के घेर लिया था जिके छः महीने बाद जेरूशलम शासक पैट्रिआर्क सोफ्रोनियस ने इस्लामी सेना को जेरूशलम में प्रवेस करने पर सहमति जताई जिसके बाद खलीफा उमर ने  ईस्वी में शहर को प्रस्तुत करने के लिए जेरूशलम की यात्रा की इसी तरह जेरूशलम शासक पेट्रिआर्क ने खलीफा उमर को आत्मसमर्पण कर दिया जो जेरूशलम की मुस्लिम विजय ने फिलिस्तीन पर अरब मुस्लिम नियंत्रण को मजबूत कर दिया जो  वीं शताब्दी के अंत में तक रहा  वीं शताब्दी में ईसाई धर्म योद्धाऔ ने ने आक्रमण कर मुस्लिमों से जीत लिया लेकिन कुछ समय के बाद उन्हें जेरूशलम से कुर्द शासक सलाउद्दीन के रूप में हार का समना करना पड़ा जो पुनः जेरूशलम मुस्लिमों के अधीन हो गया#  -  @jerusalama ki gherabandi siege of jerusalem jerusalama ki gherabandi eka sainya sangharsa ka hisa thi jo vijantina samrajya aura rasiduna khilapha ke bica  isvi mem hui thi yaha jaba suru hui jaba rasiduna sena ki kamana abu ubaidaha ke tahata navambara  isvi mem jerusalama ke ghera liya tha jike chah mahine bada jerusalama sasaka paitriarka sophroniyasa ne islami sena ko jerusalama mem pravesa karane para sahamati jatai jisake bada khalipha umara ne  isvi mem sahara ko prastuta karane ke lie jerusalama ki yatra ki isi taraha jerusalama sasaka petriarka ne khalipha umara ko atmasamarpana kara diya jo jerusalama ki muslima vijaya ne philistina para araba muslima niyantrana ko majabuta kara diya jo  vim satabdi ke anta mem taka raha  vim satabdi mem isai dharma yoddhaon ne akramana kara muslimom se jita liya lekina kucha samaya ke bada unhem jerusalama se kurda sasaka salauddina ke rupa mem hara ka samana karana para jo punah jerusalama muslimom ke adhina ho gaya#\n",
            "Skipping:  32 33 @इसके अतिरिक्त पूरे विश्व में जहाँजहाँ मनुष्य गया इसने उनका अनुकरण किया और अमरीका के अधिकतर स्थानों अफ्रीका के कुछ स्थानों न्यूज़ीलैंड और आस्ट्रेलिया तथा अन्य नगरीय बस्तियों में अपना घर बनाया#  -  @iske atirikt pure vishwa men jahanjahan manushya gaya isne unka anusaran kiya aur america ke adhikar sthanon africa ke kuch sthanon new zealand aur australia tatha anya nagariy bastiyon men apna ghar banaya#\n",
            "Skipping:  26 25 @यह अमरीका के आमेज़न नदी के जंगलों में उगता था और अब भारत के त्रावणकोर कोचीन मैसूर मलाबार कुर्ग सलेम और श्रीलंका में उगाया जाता है#  -  @yah america ke aamezon nadi ke janglon men ugta tha aur ab bharat ke travankor kochin malabar kurg salem aur shrilanka men ugaya jata hai#\n",
            "Skipping:  22 21 @कांचनाबुरी दो संस्कृत शब्दों का थाई भाषा रूप है कंचन और पुरी अर्थात कांचनाबुरी का अर्थ कंचनपुरी या सोने की नगरी है#  -  @kanchanaburi do sanskrut shabdon ka thai bhasha roop hai kanchan aur puri  arthat kanchanaburika arth kanchanpuri ya sone ki nagri hai#\n",
            "Skipping:  22 21 @लेकिन छात्रों का एक समूह तूफान की तरह इमारत में चढ़ने लगा था इस कारण पुलिस ने आग के गोले फैंकने लगे#  -  @lekin chhatron ka ek samuh toofan ki imarat men chadhane laga tha is karan police ne aag ke gole faikane lage#\n",
            "Skipping:  12 11 @इसमें भिन्न प्रकार की सीलें जलव्याघ्र सी लायन वाल्रस इत्यादि शामिल हैं#  -  @ismen bhinn prakar kee silen jalvyaghr see layan varlasityadi shamil hain#\n",
            "Skipping:  11 10 @सेवापेट रोड तमिल नाडु की राजधानी चेन्नई का एक क्षेत्र है#  -  @sevapet rod tamil naadu rajdhani chennai ka ek kshetr hai#\n",
            "Skipping:  26 25 @अनन्त राव सार देसाई जो कि मराठी और पुर्तगाली भाषाओ के रेडियों के नाटककार और लघुकथा लेखक जो मार्दोल में रहे और औषधिशास्त्र का अभ्यास किया#  -  @anant rao sardesai jo ki marathi aur purtgali bhasao ke radio ke natkaar aur laghukatha lekhak jo maardol mein rehe aur ausadhishastra ka abhyaas kiya#\n",
            "Skipping:  86 87 @हालांकि जैसा कि ऊपर उल्लेख किया गया है ब्रह्म सूत्र की शंकरकी व्याख्या की तरह माधव इस पर सहमत हैं कि उनके द्वारा किये गये अच्छे और बुरे कर्मों के अनुरूप जीवों को पुरस्कार और दंड भगवान द्वारा प्रदान किया जाता है और वह अपनी इच्छा से न्याय पर खुद को दृढ रखते हुए ऐसा करता है और मनुष्य के कर्मों द्वारा उसके कार्यों को नियंत्रित नहीं किया जा सकता और न ही उस पर पक्षपात या किसीके साथ क्रूरता का आरोप लगाया जा सकता है#  -  @halanki jaisa ki upar ullekh kiya gaya hai brahma sutra ki shankar ki vyakhya ki tarah madhav is par sehmat hai ki unke dvara kiye gaye acche aur bure karmo ke anurup jivo ko puraskar aur dand bhagvan dvara pradan kiya jata hai aur vah apni iccha se nyay par khud ko dridh rakhte hue aisa karta hai aur manushya ke karmo dvara uske karyo ko niyantrit nahi kiya ja sakta aur na hi us par pakshapaat ya kisike sath krurta ka aarop lagaya ja sakta hai#\n",
            "Skipping:  70 68 @संपत्ति अंतरण अधिनियम  की धारा a में बंधक mortgage को परिभाषित किया गया है i जिसके अनुसार बंधक का तात्पर्य किसी ऋण के रूप में अग्रिम तौर पर ली गई या ली जाने वाली राशी अथवा कोई विधमान या भविष्य ऋण अथवा किसी कार्य व्यवहार से उत्पन्न आर्थिक देनदारियों के दायित्व के भुगतान की सुरक्षा के उद्देश्य के लिए किसी विनिर्दिष्ट अचल संपत्ति में हित के अंतरण से हैं i#  -  @sampatti antran adhiniyam  ki dhara a mein bandhak mortgage ko paribhashit kiya gaya hai  jisake anusar bandhak ka tatparya kisi rin ke roop mein agrim taur par li gai ya ii jane vali rashee athava koi vidhman ya bhavishya rin athava kisi karya vyavhar se utpann arthik dendariyon ke dayitv ke bhugtan ki suraksha ke uddhesya ke liye kisi vinirdisht achal sampatti mein hit ke antran se hain#\n",
            "Skipping:  9 8 @नाइजीरियाआई राष्ट्रीय अन्तरिक्ष अनुसंधान एवं विकास एजेन्सी — नायजीरियासैट#  -  @nigeriai rashtriya antriksh anusandhan evam vikas ejensi  nigeriasat#\n",
            "Skipping:  37 38 @एजरिन वाटर्स के एकल एल्बम पर काम करने में असमर्थ थे और इसके बजाए उन्होंने गिल्मर के साथ काम करने का फैसला किया  डेव और मेरे लिए फ़्लॉइड रिकॉर्ड का हमारा संस्करण बनाना अपेक्षाकृत काफी आसान है#  -  @yejrin waters ke yekal album par kaam karne men asmarth the aur iske bajaye unhone gilmar ke sath kaam karne ka faisla kiya … dev aur mere liye flaid record ka hamara sanskaran banana apekshakrut kafi aasan hai#\n",
            "Skipping:  26 25 @महीने के भीतर एंटोनेस्क्यु antonescu ने आयरन गार्ड को कुचल डाला और अगले वर्ष में रोमानिया अक्षीय शक्तियों की तरफ से युद्ध में शामिल हो गया#  -  @mahine ke bhitar antonescu ne iron guard ko kuchal dala aur agle varsh men romania alshiiy shaktiyon ki taraf se yuddh men shamil ho gaya#\n",
            "Skipping:  30 31 @परीक्षा के पत्रों की संख्या तो  ही रही किंतु दूसरा पत्र ऐच्छिक जिसमें विभिन्न विषयों के छात्र अपनी रुची के अनुसार अलगअलग विषयों का चुनाव करते थे समाप्त करदिया गया#  -  @parikshaa ke patron ki sankhayaa to  hi rahi kintu dusaraa ptra aichchhik jismein vibhinn vishayon ke chhaatr apni ruchi ke anusaar alagalaga visyon ka chunaav karte the samaapt kar diya gaya#\n",
            "Skipping:  12 13 @ओलीटिक्स शब्दावली यिनके राजनैतिक दृढता एवम् राष्ट्रिय प्रतिज्ञाके खातिर चयन किया था#  -  @aulitiks shabdavali jinke rajnaitik dridhta evam rashtriya pratigya ke khatir chayan kiya tha#\n",
            "Skipping:  13 14 @भारत ने ऑस्ट्रेलिया और न्यूजीलैंड से जुड़े त्रिकोणीय वनडे टूर्नामेंट में हिस्सा लिया#  -  @bharat ne australia aur new zealand se jude trikoniy oneday tournament men hissa liya#\n",
            "Skipping:  13 12 @इसमें बीबीक्यू bbq सॉस जोड़कर जर्मनी में स्थायी रूप से बेचा जाता है#  -  @ismen bbq sauce jodkar germany mein sthayi rup se beja chata hai#\n",
            "Skipping:  7 6 @जो बिहारी एक तरफ लिखते हैं —#  -  @jo bihari ek taraf likhate hain#\n",
            "Skipping:  9 8 @डॉली चावला — ज्योति शंकर सुराना विजय की पत्नी#  -  @dolly chawla  jyoti shankar surana vijay ki patni#\n",
            "Skipping:  14 12 @कैरी ने  में युक् युक् टोरंटो ओंटारियो से कॉमेडी की शुरू आत की थी#  -  @carrey ne  mein uk toronto  ontario se comedy ki shuruaat ki thi#\n",
            "Skipping:  12 11 @रेवा rewa प्रशांत महासागर में स्थित फ़िजी देश का एक प्रान्त है#  -  @rewa rashant mahasagar men sthit fiji desh ka ek prant hai#\n",
            "Skipping:  15 16 @बोगरा का युद्ध भारत पाक युद्ध  का भाग था जो वर्तमान बांग्लादेश में हुआ था#  -  @bogra ka yudh bharat paak yudh १९७१ ka bhag tha jo vartaman bangladesh mein hua tha#\n",
            "Skipping:  16 17 @शिल्प और भाषा की दृष्टि से भी प्रेमचंद ने हिन्दी उपन्यास को विशिष्ट स्तर प्रदान किया#  -  @shilp aur bhasha ki drushti se bhi premchand ne hindi upnyaas ko vishisht star pradan kiya …#\n",
            "Skipping:  28 27 @एक अन्य परंपरा कालिदास के अभिज्ञान शाकुंतल से इस प्रकार मिलती है कि दुष्यंत के वंशज कुरु थे जिनके दूसरे पुत्र की शाखा में राजा उपरिचरवसु हुए थे#  -  @ek any parampara kalidaas ke  abhbihgyan shakuntal se is prakar milti hai dushyant ke vanshaj kuru the jinke dusre putr ki shakha men raja upricharwasu huye the#\n",
            "Skipping:  21 22 @कानपुर में हिन्दूमुस्लिम दंगा भड़क उठा जिसे शान्त करवाने की कोशिश में क्रान्तिकारियों के शुभचिन्तक गणेशशंकर विद्यार्थी भी शहीद हो गये#  -  @kanpur mein hindumuslim danga bhadak utha jise shaant karvaane ki koshish mein krantikariyon ke shubhchintak ganesh shankar vidyarthi bhi shaheed ho gaye#\n",
            "Skipping:  5 6 @इसके लेखकलाला श्रीनिवास दास हैं#  -  @iske lekhak lala shrinivaas das hain#\n",
            "Skipping:  23 22 @कई कुत्तों के बालों के मिश्रित पैटर्न होते हैं पूरे शरीर पर घुंघराले बाल लेकिन पूंछ और कान पर लहराते बाल होते हैं#  -  @kai kutton ke balon ke mishrit patern hote hain pure sharir par ghungrale baal lekin punchh aur kaan lahraate baal hote hain#\n",
            "Skipping:  19 20 @जब पैर में काँटा चुभता है तब आखोँ से पानी आता है और हाथ काँटा निकालनेके लिए जाता है#  -  @jab pair mein kaantaa chubhataa hai tab aakhon se pani aataa hai aur haath kaantaa nikalane ke liye jata hai#\n",
            "Skipping:  29 28 @यही नहीं इन प्रतिरूपों द्वारा क्रियावान पुरु ष के उन गणितीय संकल्पनाओं का बोध कराया जा सकता है जो गणित की अपूर्व प्रतिभावाले व्यक्ति के लिए स्वत बोधगम्य हों#  -  @yahi nahi in pratiroopon dvara kiryavaan purush ke un ganitiya sankapnaon ka bodh karaya jaa sakta hai jo ganit ki apurva pratibhavale vyakti ke liye swatah bodhgamya hon#\n",
            "Skipping:  25 24 @लेफ्टिनेंट उमर फ़ैयाज़ परे का  मई  की रात को जम्मू और कश्मीर के शोपियां जिले में उनके रिश्तेदार के घर से अपहरण कर लिया गया#  -  @lieunent omar faiyaz pare ka  may  kii raat jammu aur kashmir ke shopiyan jile men unke rishtedar ke ghar se apharan kar liya gaya#\n",
            "Skipping:  22 21 @स्नायु व्यायाम या न्युरोबिक्स neurobics मानसिक व्यायामों को कहते हैं जिनके करने से मस्तिष्क की क्षमता बढ़ने का दावा किया जाता है#  -  @snayu vyayam ya neurobics manasik vyayamon ko kahate hain jinke karne se mastishaka ki kshamata badhane ka dava kiya jata hai#\n",
            "Skipping:  19 20 @गंगोत्री ग्लेशियर संस्कृत नेपाली और हिन्दी गंगोत्री उत्तरकाशी जिला उत्तराखण्ड भारत में स्थित हैजिसकी सीमा तिब्बत से लगती है#  -  @gangotri glacier sanskrut nepali aur hindi gangitri uttarkashi zilla uttarakhand bharat men sthit hai jiski sima tiber se lagti hai#\n",
            "Skipping:  8 7 @लिली पटेल — सावित्री देवी जया की नानी#  -  @lili patel  savitri devi jaya ki nani#\n",
            "Skipping:  10 11 @वीडियो यू ट्यूब पर देखेंथानेश्वर महादेव मन्दिर कुरुक्षेत्र की यात्रा#  -  @vidiyo you tube pr dekhe thaneshwar mahadev mandir kurukshetra ki yatra#\n",
            "Skipping:  10 9 @तरुणा निरंकारी — रिचा शर्मा जया की दूसरी बड़ी बहन#  -  @taruna nirankari  richa sharma jaya ki dusri badi bahan#\n",
            "Skipping:  41 42 @ई सं  के लगभग बहुत से वैज्ञानिकों को विश्वास था कि विमिय एमसिद्धांत जिसे पाँच पर्टर्बेटिव विचलन परमस्ट्रिंग सिद्धांतों में से एक द्वारा कुछ सीमाओं के द्वारा व्याख्या की जाती है और अन्य अधिकतम परमसममितिय  विमिय परमगुरुत्व द्वारा सर्वतत्व सिद्धांत है#  -  @es  ke lagbhag bahut se vigyanikon ko vishwas that ki vimiya m siddhant jise panch perturbitive vichalan permstring siddhanton men se ek dwara kuch simaon ke dwara vyakhya ki jati hai aur anya adhiktam paramsammitay vimiya paran gurutwa dwara sarvatatva siddhant hai#\n",
            "Skipping:  11 10 @तीन वर्ष से ही बच्चों की भाषा का विकास होता है#  -  @teen varsh se bacchhon ki bhasha ka vikaas hota hai#\n",
            "Skipping:  10 11 @प्रधानमंत्री टॉम थाबाने सरकार के मुखिया है और कार्यकारी अधिकारहै#  -  @pradhanmantri tom thabane sarkar ke mukhiya hai aur karyakari adhikar hai#\n",
            "Skipping:  21 20 @संयोग से फारसी शब्द वार आक्रमण और अंग्रेजी शब्द war वार में उच्चारण और रूपसाम्य ही नहीं युद्धविषयक भावसाम्य भी है#  -  @sanyog se farsi shabd var akraman aur angreji shabd war men uchharan aur rupsamya hi nahi yuddhavishayak bhavsamya bhi hai#\n",
            "Skipping:  22 23 @में न्यूज़ीलैंड में अपनी अंतिम पारी में मैच जीतने वाले शानदार  रन बनाने के बाद उन्होंने अंतरराष्ट्रीय क्रिकेट से संन्यास ले लिया#  -  @men new zealand men apni antim pari men match jitne wale shandar  ran banane ke baad unhone antarrashtriy cricket se sannyas le liya#\n",
            "Skipping:  36 34 @शेयरसन लोएब रोड्स स्वयं ही  के दशक में कई विलयों का जनक था जिनमें  का वेइल्स हेडन स्टोन एंड कंपनी का शेयरसन हेडन स्टोन से शेयरसन हैम्मिल एंड कंपनी के साथ किया गया विलय शामिल है#  -  @sheyson loab rhods swaym hi  ke dashak men kaii vilayon ka janak tha jinmen  ka wedls heden stone  company ka sheyarson heden stone se sheyarson hammil  company ke sath kya gya vilay shamil hai#\n",
            "Skipping:  8 9 @वेस्टइंडीज टॉस जीता और मैदान पर चुने गए#  -  @west indies toss jeeta aur maindan par chune gaya#\n",
            "Skipping:  32 30 @गोया विशेषज्ञ पियरे गेसिअर  के अनुसार गोया की व्यक्तिगत एल्बम चित्रकला के केटलॉग रेसोने में गोया प्रत्यक्ष रूप से गुलिवर ट्रेवल्स के अध्याय i के भाग i से प्रेरित दिखाई देते हैं#  -  @goya vishash piyre gesiar  ke anusaar goya ki vyaktigat album chitrkala ke ketlog resone men goya pratyaksh roop se guliwar travals kee adhyay  ke bhag  se prerit dikhai dete hain#\n",
            "Skipping:  17 18 @में राबिनको शिमॉन पेरेझ और यासिर अराफ़ात के साथ नोबेल शांति पुरस्कार से सम्मानित किया गया था#  -  @mein robin ko shimon peres aur yasser arafat ke saath nobel shaanti puraskaar se sammanit kiya gaya tha#\n",
            "Skipping:  20 19 @गांगुली क्षितींद्र नाथ मजूमदार यामिनी राय तथा गगनेंद्र नाथ टैगोर के चित्रों का संग्रह इस संग्रहालय की अमूल्य निधि है#  -  @gangulishitindra nath majumdar yamini rai tatha gagnendra nath tagore ke chitron ka sangrah is sangralay ki amulya nidhi hai#\n",
            "Skipping:  11 10 @इसके तीन देष्काणों में उनके स्वामी ’शुक्रशुक्र” शुक्रबुध’ और शुक्रशनि हैं#  -  @iske teen deshkanon men unke swami shukrshukr  shukrbudhaur shukrshani hain#\n",
            "Skipping:  7 6 @सृष्टि जैन — जया समर की पत्नी#  -  @srushti jain  jaya samar ki patni#\n",
            "Skipping:  23 22 @बड़े उत्तरी मांसाहारी जानवरों जैसे वॉलरस आदि की एक चमकाई हुई और नक़्क़ाशीदार स्तंभास्थि को अलास्काई संस्कृति मे उसिक oosik कहा जाता है#  -  @bade uttari mansahari janvaron jaise walrus aadi ki ek chamkai hui aur makkashidar stambhsthi ko alaskai sanskruti men oosik kaha jata hai#\n",
            "Skipping:  27 28 @न्यूज़ीलैंड बैंकर्स एसोसिएशन उद्योग मानकों और कुछ मामलों में नीतियां स्थापित करता है लेकिन भुगतान सेवा संस्थाओं की अपनी शासन व्यवस्था व्यापार रणनीतियां और नियमें होती हैं#  -  @new zealand bankers association udyog manakon aur kuch mamlon men nitiyan sthapit karta hai lekin bhugtan seva sanasthaon ki apne shasan vyavastha vyapar rannitiyan aur niyamen hoti hain#\n",
            "Skipping:  27 26 @जोहड़बांध नदी के पुनर्जीवन कार्य में प्रयास करने वाली संस्थाएं तरुण भारत संघ राजीव गांधी फाउण्डेशन जल बिरादरी के साथ सपोटरा डांग की  ग्राम सभाओं के ग्रामवासी#  -  @johadbandh  nadi ke punrjivan kary men prayaas wali sansthayen tarun bharat sangh rajiv gandhi foundation jal biradari ke saath sapotara dang ki  gram sabhaon ke gramwasi#\n",
            "Skipping:  8 7 @सेना में पैदल अश्वारोही और हाथी होते थे#  -  @sena men paidal ashwaarohi aur hathi the#\n",
            "Skipping:  29 28 @यह विशेषकर विंडोज़  के लिये बनाया गया है हालाँकि यह विंडोज़ ऍक्सपी पर भी चलता है परन्तु ऍक्सपी के लिये यह इण्डिक ऍक्सपी प्लस में पहले से शामिल है#  -  @yah visheshkar windows  liye banaya gaya hai halanki yah windows xp par bhi chalta hai parantu xp ke liye yah indic xp plus men pahle se shamil hai#\n",
            "Skipping:  21 20 @दारु या जाइलम xylem पौधों में पाये जाने वाले दो संवहन ऊतको में से एक है दूसरा संवहन ऊतक फ्लोएम है#  -  @daru ya xylem paudhom men paye jane wale do sanvahan utakon men se ek hai dusra sanvahan utak floem hain#\n",
            "Skipping:  22 21 @रूपेश ने रुतु के साथ से शादी की है उनके पिता और चाचा रूपेश की तरह वह एक चित्रकार और कार्टूनिस्ट है#  -  @rupesh ne ritu ke sath shadi ki hai unake pita aur chacha rupesh ki tarah vah ek chitrakar aur kartunist hai#\n",
            "Skipping:  3 4 @भारतीय शिक्षाका इतिहास#  -  @bhartiya shiksha ka itihas#\n",
            "Skipping:  9 8 @माही शर्मा — शिखा शर्मा जया की बड़ी बहन#  -  @mahi sharma  sikha sharma jaya ki badi behan#\n",
            "Skipping:  42 43 @पवित्र उपवन  यह एक पर्यावरणीय संकल्पना है  इसे अंग्रेजी भाषा में sacred grove सेक्रेड ग्रोव थी मराठी भाषा में देवराई नामसे जाना जाता है  भारतमें तथा विदेश में भी इस उपवन की संकल्पना है  विदेशमें इसे चर्च फॉरेस्ट नामसे जाना जाता है#  -  @pavitra upvan  yah ek paryavaraniya sankalpna hai  ise angreji bhasha me sacred grove sacred grove thi marathi bhasha me devrai namse jana jata hai  bharat me tatha videsh me bhi is upvan ki sankalpna hai videsh mem isechurch forestnam se jana jata hai#\n",
            "Skipping:  19 20 @जानकी मन्दिर नेपाली  जानकी मन्दिर नेपाल के जनकपुर के केन्द्र में स्थित एक हिन्दू मन्दिर एवं ऐतिहासिक स्थल है#  -  @janki mandir nepali janki mandir mepal ke janak pur ke kendra men sthit ek hindu mandir evam aitihasik sthal hai#\n",
            "Skipping:  20 21 @जून  को भोपाल में आयोजित समारोह में मध्यप्रदेश के तत्कालीन मुख्यमंत्री कैलाश जोशी ने श्रीराय को ये सम्मान प्रदान किया#  -  @june  ko bhopal men ayojit samaroh men madhya pradesh ke tatkalin mukhyamantri kailas joshi ne shrirai ko ye samman pradan kiya#\n",
            "Skipping:  9 8 @हेमंत चौधरी — गौरी शंकर सुराना समर के पिता#  -  @hemant chaudhari  gauri shankar surana samar ke pita#\n",
            "Skipping:  10 9 @प्रिओम गुज्जर — विजय शंकर सुराना समर का चचेरा भाई#  -  @priyom gujjar  vijay shankar surana samar ka chacera bhai#\n",
            "Skipping:  59 58 @कुड़मी सम्पूर्ण झारखंड  जिला छोड़कर बंगाल के पुरूलिया बांकुड़ा व मिदनापुर उड़ीसा के क्योंझर मयूरभंज व सुंदरगढ वृहत् झारखंड क्षेत्र तथा असम एवं छत्तीसगढ़ के कुछ क्षेत्रों में निवासरत टोटेमिक गुस्टिधारी कुर्मी मूलत द्रविड़ प्रजाति के आदिवासि समुदाय के लोग हैं जिनकी अपनी स्वायत्त व समृद्धशाली भाषा कुड़मालि है एवं विशिष्ट आदि सभ्यतासंस्कृति परंपरा व रीतिरिवाजों के धारकवाहक हैं#  -  @kudmi sampurn zarkhand  jila chodkar bangal ke puruliya bankuda v midnapur ke kyojar mayurbhanj v sundargadh vakrut zarkhand shetra tatha asam yeon chhattisgadh ke kuch shetron men nivasarat  totemik gushtidhari kurmi mulat dravid prajati ke adiwasi samuday ke log hai jinki apni swayatt v samruddhshali bhasha kudmali hai yeon vishisht aadi sabhyatasanskruti parampara v ritiriwajo ke dharakvahak hain#\n"
          ]
        }
      ],
      "source": [
        "train_data, train_sents = create_dataset(l_train)\n",
        "test_data, test_sents = create_dataset(l_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDp--DDk22_h",
        "outputId": "27f2fa56-1075-415c-e53c-d21c1af78ab0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['@iske', 'aane', 'se', 'purva', 'hi', 'log', 'gharon', 'ki', 'safai', 'ka'],\n",
              " ['@इसके', 'आने', 'से', 'पूर्व', 'ही', 'लोग', 'घरों', 'की', 'सफाई', 'का'])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[0][:10],train_data[1][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDX5wiBy28vJ",
        "outputId": "dce033c4-1614-4a27-97d2-df7dbe6825cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['@kumbh',\n",
              "  'rashi',\n",
              "  'men',\n",
              "  'janme',\n",
              "  'log',\n",
              "  'sambhavnaon',\n",
              "  'se',\n",
              "  'bhari',\n",
              "  'ek',\n",
              "  'jagah'],\n",
              " ['@कुंभ',\n",
              "  'राशि',\n",
              "  'में',\n",
              "  'जन्मे',\n",
              "  'लोग',\n",
              "  'संभावनाओं',\n",
              "  'से',\n",
              "  'भरी',\n",
              "  'एक',\n",
              "  'जगह'])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data[0][:10],test_data[1][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Soj7fLrc4K1N",
        "outputId": "bb614c79-8e08-4c35-f035-c8f4520315c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(85161, 85161, 86526, 86526)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(test_data[0]),len(test_data[1]),len(train_data[0]),len(train_data[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1qKXvZt4Whe"
      },
      "source": [
        "Combine the test and train data. So that we can split train and test set according to our needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO50_sOR4R-5",
        "outputId": "81a15616-6f0b-43da-deec-4e52fe7a50d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "171687 171687\n"
          ]
        }
      ],
      "source": [
        "train_data[0].extend(test_data[0])\n",
        "train_data[1].extend(test_data[1])\n",
        "\n",
        "print(len(train_data[0]),len(train_data[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFOQQuTH4Yv7",
        "outputId": "fd9ec532-d09e-4cf5-cb02-0c8177173681"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('malhotra#', 'मल्होत्रा#')"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[0][-1],train_data[1][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmzafDk24aYE",
        "outputId": "59c230dc-c299-4669-ca02-0983d2b31349"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(171687, 171687)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_data[0]),len(train_data[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-35MYkf4o8e"
      },
      "source": [
        "Vocab class. This class will store word2index and index2word mapping for both language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dMX2bnuN4cMZ"
      },
      "outputs": [],
      "source": [
        "class WordIndex():\n",
        "  def __init__(self, lang):\n",
        "    self.lang = lang\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = {}\n",
        "    self.vocab = set()\n",
        "    \n",
        "    self.create_index()\n",
        "    \n",
        "  def create_index(self):\n",
        "    for phrase in self.lang:\n",
        "      for l in phrase:\n",
        "        self.vocab.update(l)\n",
        "    \n",
        "    self.vocab = sorted(self.vocab)\n",
        "    \n",
        "    self.word2idx['<pad>'] = 0\n",
        "    for index, word in enumerate(self.vocab):\n",
        "      self.word2idx[word] = index + 1\n",
        "    \n",
        "    for word, index in self.word2idx.items():\n",
        "      self.idx2word[index] = word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93ermi8R4rCH"
      },
      "source": [
        "Helper functions to create Lang vocab and tensors from words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tPDET0wv4tVp"
      },
      "outputs": [],
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "\n",
        "def load_dataset(pairs):\n",
        "\n",
        "\n",
        "    # index language using the class defined above    \n",
        "    inp_lang = WordIndex(pairs[0])\n",
        "    targ_lang = WordIndex(pairs[1])\n",
        "    \n",
        "    # Vectorize the input and target languages\n",
        "    \n",
        "    # English words\n",
        "    input_tensor = [[inp_lang.word2idx[s] for s in en] for en in pairs[0]]\n",
        "    \n",
        "    # hindi words\n",
        "    target_tensor = [[targ_lang.word2idx[s] for s in hn] for hn in pairs[1]]\n",
        "    \n",
        "    # Calculate max_length of input and output tensor\n",
        "    # Here, we'll set those to the longest sentence in the dataset\n",
        "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
        "    \n",
        "    # Padding the input and output tensor to the maximum length\n",
        "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
        "                                                                 maxlen=max_length_inp,\n",
        "                                                                 padding='post')\n",
        "    \n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
        "                                                                  maxlen=max_length_tar, \n",
        "                                                                  padding='post')\n",
        "    \n",
        "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "iAb8Lyf94znH"
      },
      "outputs": [],
      "source": [
        "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ec3g-Dz5Srk"
      },
      "source": [
        "Function to convert tensor to word and print."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ka4fSi1a5TuR"
      },
      "outputs": [],
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.idx2word[t]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJSdV97v5Vkc",
        "outputId": "24fec5c3-9284-4bec-b2ec-9275ee403d1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15 ----> m\n",
            "3 ----> a\n",
            "14 ----> l\n",
            "10 ----> h\n",
            "17 ----> o\n",
            "22 ----> t\n",
            "20 ----> r\n",
            "3 ----> a\n",
            "1 ----> #\n",
            "70 ----> म\n",
            "73 ----> ल\n",
            "96 ----> ्\n",
            "80 ----> ह\n",
            "94 ----> ो\n",
            "60 ----> त\n",
            "96 ----> ्\n",
            "72 ----> र\n",
            "82 ----> ा\n",
            "1 ----> #\n"
          ]
        }
      ],
      "source": [
        "convert(inp_lang,input_tensor[-1])\n",
        "convert(targ_lang,target_tensor[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZYUGjVv5ZUj"
      },
      "source": [
        "Train and Test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB3dTfuH5XY_",
        "outputId": "607d96d8-5288-472a-b059-b1aa6693dc7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(90000, 90000, 10000, 10000)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_tensor = input_tensor[:100000]\n",
        "target_tensor = target_tensor[:100000]\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.1)\n",
        "\n",
        "# Show length\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffPl-hNJ5fD9"
      },
      "source": [
        "Some hyper-parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "B22OLUej5dSa"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 128\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word2idx)+1\n",
        "vocab_tar_size = len(targ_lang.word2idx)+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Jo6CXSgT5hJt"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMNyZQab5jh6",
        "outputId": "44e6d1a0-f296-4441-8abf-cb984686c0bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TensorShape([128, 35]), TensorShape([128, 32]))"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHE32ts35mlh"
      },
      "source": [
        "Let's build the model.\n",
        "The encoder : it will consume input and produce a thought vector which will be the input of the decoder after applying attention to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "oBVNw0x95lUx"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "#     print(type(x))\n",
        "    x = self.embedding(x)\n",
        "#     print(x.shape)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0CkK3WK5rVy",
        "outputId": "004b9924-04b6-42d6-c958-122ddd6e7c76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (128, 35, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (128, 1024)\n"
          ]
        }
      ],
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRQMxFrq5v5_"
      },
      "source": [
        "Attention part : this model will return weights so that we feed them into decoder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ay-H51HW5t66"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do5n1glj5x0L",
        "outputId": "df74d224-10c0-414f-9049-1a629a38b0c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (128, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (128, 35, 1)\n"
          ]
        }
      ],
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtmxfsMV50T5"
      },
      "source": [
        "The decoder : calculates the probablity of next character by taking input the attention weights and encoder input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "BPziPaJu51FO"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHJut2_Y53Nc",
        "outputId": "af84358d-42e9-4724-d9b7-8fc81e05c7bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (128, 111)\n"
          ]
        }
      ],
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((128, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "_TLy2S8z550n"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJVu3GMH57Xx"
      },
      "source": [
        "Adding checkpoints to store the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "skKWM_Eb59hS"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7YkujRW5_xs"
      },
      "source": [
        "Training step function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "sino4tbT5_Lo"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word2idx['@']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2eQ0uoW-Sa2",
        "outputId": "5f211236-afb0-4712-da54-f1146029c5a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "703\n"
          ]
        }
      ],
      "source": [
        "print(len(dataset.take(steps_per_epoch)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y17yLhZS6DtP"
      },
      "source": [
        "Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEXHlXEx6FZp",
        "outputId": "47de9f75-f557-48db-9bee-f811b870252e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss 0.1391\n",
            "Time taken for 1 epoch 673.3607249259949 sec\n",
            "\n",
            "Epoch 2 Loss 0.0458\n",
            "Time taken for 1 epoch 622.4627146720886 sec\n",
            "\n",
            "Epoch 3 Loss 0.0358\n",
            "Time taken for 1 epoch 618.4995405673981 sec\n",
            "\n",
            "Epoch 4 Loss 0.0282\n",
            "Time taken for 1 epoch 621.2075388431549 sec\n",
            "\n",
            "Epoch 5 Loss 0.0295\n",
            "Time taken for 1 epoch 621.9276003837585 sec\n",
            "\n",
            "Epoch 6 Loss 0.0211\n",
            "Time taken for 1 epoch 619.9608371257782 sec\n",
            "\n",
            "Epoch 7 Loss 0.0179\n",
            "Time taken for 1 epoch 618.409530878067 sec\n",
            "\n",
            "Epoch 8 Loss 0.0158\n",
            "Time taken for 1 epoch 622.3090085983276 sec\n",
            "\n",
            "Epoch 9 Loss 0.0166\n",
            "Time taken for 1 epoch 605.6216759681702 sec\n",
            "\n",
            "Epoch 10 Loss 0.0150\n",
            "Time taken for 1 epoch 607.9602472782135 sec\n",
            "\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "loss_values = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "  \n",
        "    ep_loss = total_loss / steps_per_epoch\n",
        "    loss_values.append(ep_loss)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, ep_loss))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "wz8Wy_jaI6oN",
        "outputId": "c82cd297-39ad-4071-b949-5218450ac8be"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(loss_values)\n",
        "plt.title('Train Loss vs Epoch')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RglGvLH1Naaf",
        "outputId": "d8e2c29b-182a-469c-8fa7-4f0ce0501d35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/training_checkpoints/ (stored 0%)\n",
            "  adding: content/training_checkpoints/ckpt-4.data-00000-of-00001 (deflated 22%)\n",
            "  adding: content/training_checkpoints/ckpt-2.data-00000-of-00001 (deflated 22%)\n",
            "  adding: content/training_checkpoints/ckpt-1.index (deflated 70%)\n",
            "  adding: content/training_checkpoints/ckpt-4.index (deflated 70%)\n",
            "  adding: content/training_checkpoints/ckpt-3.data-00000-of-00001 (deflated 22%)\n",
            "  adding: content/training_checkpoints/ckpt-2.index (deflated 70%)\n",
            "  adding: content/training_checkpoints/ckpt-3.index (deflated 70%)\n",
            "  adding: content/training_checkpoints/ckpt-5.data-00000-of-00001 (deflated 22%)\n",
            "  adding: content/training_checkpoints/ckpt-5.index (deflated 70%)\n",
            "  adding: content/training_checkpoints/ckpt-1.data-00000-of-00001 (deflated 22%)\n",
            "  adding: content/training_checkpoints/checkpoint (deflated 38%)\n"
          ]
        }
      ],
      "source": [
        "# !zip -r /content/drive/MyDrive/NLP/Project/outputs/checkpoints_att_eng-hin.zip /content/training_checkpoints\n",
        "!zip -r /content/drive/MyDrive/NLP/checkpoints_att_eng-hin.zip /content/training_checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVr8GZyhJLUs"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "vYjTvLQKLUqT"
      },
      "outputs": [],
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "    sentence = preprocess_sentence_native(sentence)\n",
        "\n",
        "    inputs = [inp_lang.word2idx[i] for i in sentence]\n",
        "#     print(inputs)\n",
        "\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "#     print(inputs)\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word2idx['@']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.idx2word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.idx2word[predicted_id] == '#':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Nm3-ByitJQdE"
      },
      "outputs": [],
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "#     print(predicted_sentence)\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "6fItdhWcJTBD"
      },
      "outputs": [],
      "source": [
        "def transliterate(sentence):\n",
        "    all_words = sentence.split()\n",
        "    final_result = \"\"\n",
        "    for word in all_words:\n",
        "        result, word, attention_plot = evaluate(word)\n",
        "        final_result += \"\".join(result.split(\" \"))+\" \"\n",
        "\n",
        "        # print('Input: %s' % (word))\n",
        "        # print('Predicted translation: {}\\n'.format(''.join(result.split(' '))))\n",
        "\n",
        "\n",
        "        # result = unicode_to_ascii(result)    \n",
        "        attention_plot = attention_plot[:len(result.split(' ')), :len(list(word))]\n",
        "        # plot_attention(attention_plot, list(word), result.split(' '))\n",
        "\n",
        "    return final_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "1ATzpVcuS-58"
      },
      "outputs": [],
      "source": [
        "def calc_accuracy(test_data, n):\n",
        "    correct = 0\n",
        "    for i in range(len(test_data[0][:n])):\n",
        "        hindi, eng = test_data[0][i], test_data[1][i]\n",
        "\n",
        "        res, _, _ = evaluate(hindi)\n",
        "        res = ''.join(res.split(' '))\n",
        "\n",
        "        eng = eng.replace(\"@\", \"\")\n",
        "        eng = eng.replace(\"#\", \"\")\n",
        "        res = res.replace(\"@\", \"\")\n",
        "        res = res.replace(\"#\", \"\")\n",
        "\n",
        "        # print(eng, res)\n",
        "        if eng == res:\n",
        "            # print(\"Yes\\n\")\n",
        "            correct += 1\n",
        "\n",
        "    # accuracy = correct / len(test_data[0])\n",
        "    accuracy = correct / n\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "kESM_2LkLHkk"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "def calc_bleu(test_sens, n):\n",
        "    bleu_score = 0\n",
        "    for i in range(len(test_sens[0][:n])):\n",
        "        inp_sent, trg_sent = test_sens[0][i], test_sens[1][i]\n",
        "\n",
        "        res = \"\"\n",
        "        for i in inp_sent:\n",
        "            res += i+\" \"\n",
        "        res = transliterate(res)\n",
        "\n",
        "        tmp = \"\"\n",
        "        for i in trg_sent:\n",
        "            tmp += i+\" \"\n",
        "        trg_sent = tmp\n",
        "        trg_sent = trg_sent.replace(\"@\", \"\")\n",
        "        trg_sent = trg_sent.replace(\"#\", \"\")\n",
        "        res = res.replace(\"@\", \"\")\n",
        "        res = res.replace(\"#\", \"\")\n",
        "\n",
        "        reference = [trg_sent.split()]\n",
        "        candidate = res.split()\n",
        "\n",
        "        smoothie = SmoothingFunction().method4\n",
        "        bleu_score += sentence_bleu(reference, candidate, smoothing_function=smoothie)\n",
        "\n",
        "    return bleu_score / n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "Y7HWQ7R_WakC"
      },
      "outputs": [],
      "source": [
        "def calc_rouge(test_sents, n):\n",
        "    precision, recall, f1score = 0, 0, 0\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
        "    \n",
        "    for i in range(len(test_sents[0][:n])):\n",
        "        inp_sent, reference = test_sents[0][i], test_sents[1][i]\n",
        "\n",
        "        res = \"\"\n",
        "        for i in inp_sent:\n",
        "            res += i+\" \"\n",
        "        res = transliterate(res)\n",
        "\n",
        "        tmp = \"\"\n",
        "        for i in reference:\n",
        "            tmp += i+\" \"\n",
        "        reference = tmp\n",
        "        candidate = res.replace(\"@\", \"\")\n",
        "        candidate = res.replace(\"#\", \"\")\n",
        "        reference = reference.replace(\"@\", \"\")\n",
        "        reference = reference.replace(\"#\", \"\")\n",
        "        \n",
        "        tmp_p, tmp_r, tmp_f = scorer.score(reference, candidate)['rouge1']\n",
        "        precision += tmp_p\n",
        "        recall += tmp_r\n",
        "        f1score += tmp_f\n",
        "\n",
        "    precision /= n\n",
        "    recall /= n\n",
        "    f1score /= n\n",
        "    \n",
        "    return (precision, recall, f1score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGewHFOdTWNs",
        "outputId": "98977c31-d034-4dad-a9b6-c3683802714e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results on test data!!\n",
            "Accuracy on 10 test words: 0.8 \n",
            "\n",
            "Accuracy on 100 test words: 0.88 \n",
            "\n",
            "Accuracy on 500 test words: 0.788 \n",
            "\n",
            "Accuracy on 1000 test words: 0.774 \n",
            "\n",
            "Accuracy on 5000 test words: 0.7384 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Results on test data!!\")\n",
        "\n",
        "acc = calc_accuracy(test_data, 10)\n",
        "print(\"Accuracy on 10 test words:\", acc, \"\\n\")\n",
        "\n",
        "acc = calc_accuracy(test_data, 100)\n",
        "print(\"Accuracy on 100 test words:\", acc, \"\\n\")\n",
        "\n",
        "acc = calc_accuracy(test_data, 500)\n",
        "print(\"Accuracy on 500 test words:\", acc, \"\\n\")\n",
        "\n",
        "acc = calc_accuracy(test_data, 1000)\n",
        "print(\"Accuracy on 1000 test words:\", acc, \"\\n\")\n",
        "\n",
        "acc = calc_accuracy(test_data, 5000)\n",
        "print(\"Accuracy on 5000 test words:\", acc, \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YLgDha_JJ4L",
        "outputId": "1f990652-9d8e-4421-c435-33a6aab6f397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bleu score on 1000 test words: 0.6337390510214915 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "bleu = calc_bleu(test_sents, 10)\n",
        "print(\"Bleu score on 1000 test words:\", bleu, \"\\n\")\n",
        "\n",
        "# rouge = calc_rouge(test_sents, 10)\n",
        "# print(\"Rouge score on 1000 test words:\", rouge, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOvhXgt1Z6na"
      },
      "source": [
        "### Testing on various input words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSEgKxhaJVV-",
        "outputId": "8ef5e16f-d0b0-430e-90bf-78ce183201a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:  how\n",
            "Output:  हॉव# \n"
          ]
        }
      ],
      "source": [
        "inp_str = \"how\"\n",
        "out_str = transliterate(inp_str)\n",
        "print(\"Input: \", inp_str)\n",
        "print(\"Output: \", out_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1MhOmNDRwZI",
        "outputId": "74212107-28e8-46bb-c731-e2a7fe3a1e6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:  market\n",
            "Output:  मरकेट# \n"
          ]
        }
      ],
      "source": [
        "inp_str = \"market\"\n",
        "out_str = transliterate(inp_str)\n",
        "print(\"Input: \", inp_str)\n",
        "print(\"Output: \", out_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AkF7ab2b6mH",
        "outputId": "fe1ca764-652d-4556-9d00-17750245fa07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:  what\n",
            "Output:  भाट# \n"
          ]
        }
      ],
      "source": [
        "inp_str = \"what\"\n",
        "out_str = transliterate(inp_str)\n",
        "print(\"Input: \", inp_str)\n",
        "print(\"Output: \", out_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44v9SPyDcC2Z",
        "outputId": "11de9143-61e1-45f1-d989-b72784ecc557"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:  stupid\n",
            "Output:  स्टूपाइड़# \n"
          ]
        }
      ],
      "source": [
        "inp_str = \"stupid\"\n",
        "out_str = transliterate(inp_str)\n",
        "print(\"Input: \", inp_str)\n",
        "print(\"Output: \", out_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4Z8a6v7OiPa",
        "outputId": "0b584590-3714-448a-e274-26688d21e26a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:  kumbh\n",
            "Output:  कुंभ# \n"
          ]
        }
      ],
      "source": [
        "inp_str = \"kumbh\"\n",
        "out_str = transliterate(inp_str)\n",
        "print(\"Input: \", inp_str)\n",
        "print(\"Output: \", out_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhdC-9aSOmRD",
        "outputId": "b616d06a-e97b-47e7-81c5-bb026e2904e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:  raasi\n",
            "Output:  रासी# \n"
          ]
        }
      ],
      "source": [
        "inp_str = \"raasi\"\n",
        "out_str = transliterate(inp_str)\n",
        "print(\"Input: \", inp_str)\n",
        "print(\"Output: \", out_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ln-SL_YOnem",
        "outputId": "af9e37a6-d4e3-4d33-bf1b-19ca9c971eba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:  janme\n",
            "Output:  जन्मे# \n"
          ]
        }
      ],
      "source": [
        "inp_str = \"janme\"\n",
        "out_str = transliterate(inp_str)\n",
        "print(\"Input: \", inp_str)\n",
        "print(\"Output: \", out_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8NwhfscOpH6",
        "outputId": "9bb40e36-496b-40ce-f8d4-baae0a1c37ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:  log\n",
            "Output:  लोग# \n"
          ]
        }
      ],
      "source": [
        "inp_str = \"log\"\n",
        "out_str = transliterate(inp_str)\n",
        "print(\"Input: \", inp_str)\n",
        "print(\"Output: \", out_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulDDBIPt_omA",
        "outputId": "3e7d5f1e-1480-43cf-8ca4-df770e8a0d3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:  what are you doing\n",
            "Output:  भाट# आरे# यो# दोइंग#  \n",
            "\n",
            "Input:  tell me more about yourself\n",
            "Output:  ेटल# में# मोरे# अबोट# यूर्सेल्फ़फ़फ़#  \n",
            "\n",
            "Input:  go to walk\n",
            "Output:  गो# तो# वाल्क#  \n",
            "\n",
            "Input:  this is india\n",
            "Output:  दीस# इस# इंदिया#  \n",
            "\n",
            "Input:  did you have breakfast\n",
            "Output:  दीड# यो# हवे# ब्रेक्सास्ट#  \n",
            "\n"
          ]
        }
      ],
      "source": [
        "inp_str = \"what are you doing\"\n",
        "out_str = transliterate(inp_str)\n",
        "print(\"Input: \", inp_str)\n",
        "print(\"Output: \", out_str, \"\\n\")\n",
        "\n",
        "inp_str = \"tell me more about yourself\"\n",
        "out_str = transliterate(inp_str)\n",
        "print(\"Input: \", inp_str)\n",
        "print(\"Output: \", out_str, \"\\n\")\n",
        "\n",
        "inp_str = \"go to walk\"\n",
        "out_str = transliterate(inp_str)\n",
        "print(\"Input: \", inp_str)\n",
        "print(\"Output: \", out_str, \"\\n\")\n",
        "\n",
        "inp_str = \"this is india\"\n",
        "out_str = transliterate(inp_str)\n",
        "print(\"Input: \", inp_str)\n",
        "print(\"Output: \", out_str, \"\\n\")\n",
        "\n",
        "inp_str = \"did you have breakfast\"\n",
        "out_str = transliterate(inp_str)\n",
        "print(\"Input: \", inp_str)\n",
        "print(\"Output: \", out_str, \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6i6ol6eOE_0",
        "outputId": "cb9b0cdb-84e4-4839-d6df-2d40cd1b588d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:  bogra ka yudh bharat paak yudh १९७१ ka bhag tha jo vartaman bangladesh mein hua tha\n",
            "Output:  बोग्रा# का# युद# भारत# पाक# युद# # का# भग# था# जो# वर्तमान# बैंग्लादेश# में# हुआ# था#  \n",
            "\n",
            "Input:  taruna nirankari  richa sharma jaya ki dusri badi bahan\n",
            "Output:  तरुणा# निरंकरी# रिचा# शर्मा# जया# की# दूसरी# बाड़ी# बहन#  \n",
            "\n",
            "Input:  teen varsh se bacchhon ki bhasha ka vikaas hota hai\n",
            "Output:  तीन# वर्ष# से# बच्चों# की# भाषा# का# विकास# होता# है#  \n",
            "\n",
            "Input:  lili patel  savitri devi jaya ki nani\n",
            "Output:  लिली# पटेल# सवित्री# देवी# जया# की# नानी#  \n",
            "\n",
            "Input:  ismen bbq sauce jodkar germany mein sthayi rup se beja chata hai\n",
            "Output:  इसमें# bbcbbbbbbbbbbbbbbbbbbbbbbbbbbbbb सौक# जोड़कर# गेर्मीय# में# स्थायी# रूप# से# बेजा# चाता# है#  \n",
            "\n"
          ]
        }
      ],
      "source": [
        "inp_str = \"bogra ka yudh bharat paak yudh १९७१ ka bhag tha jo vartaman bangladesh mein hua tha\"\n",
        "out_str = transliterate(inp_str)\n",
        "print(\"Input: \", inp_str)\n",
        "print(\"Output: \", out_str, \"\\n\")\n",
        "\n",
        "inp_str = \"taruna nirankari  richa sharma jaya ki dusri badi bahan\"\n",
        "out_str = transliterate(inp_str)\n",
        "print(\"Input: \", inp_str)\n",
        "print(\"Output: \", out_str, \"\\n\")\n",
        "\n",
        "inp_str = \"teen varsh se bacchhon ki bhasha ka vikaas hota hai\"\n",
        "out_str = transliterate(inp_str)\n",
        "print(\"Input: \", inp_str)\n",
        "print(\"Output: \", out_str, \"\\n\")\n",
        "\n",
        "inp_str = \"lili patel  savitri devi jaya ki nani\"\n",
        "out_str = transliterate(inp_str)\n",
        "print(\"Input: \", inp_str)\n",
        "print(\"Output: \", out_str, \"\\n\")\n",
        "\n",
        "inp_str = \"ismen bbq sauce jodkar germany mein sthayi rup se beja chata hai\"\n",
        "out_str = transliterate(inp_str)\n",
        "print(\"Input: \", inp_str)\n",
        "print(\"Output: \", out_str, \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "JbAscIZeOszf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "r_5jmvMQGUyv"
      },
      "outputs": [],
      "source": [
        "result=[]\n",
        "for i in range(len(l_test)):\n",
        "    if(i==100):\n",
        "        break\n",
        "    output = transliterate(l_test[i][1])\n",
        "    temp = [l_test[i][1],output,l_test[i][0]]\n",
        "    result.append(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "Tau67IZ1GYGQ",
        "outputId": "d2a45103-ae96-46be-9229-f66c7ccd3d7a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "      <th>expected</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@kumbh rashi men janme log sambhavnaon se bhar...</td>\n",
              "      <td>कुंभ# राशि# में# जन्मे# लोग# संभवनाओंव# से# भा...</td>\n",
              "      <td>@कुंभ राशि में जन्मे लोग संभावनाओं से भरी एक ज...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@iska ulat bhi satya hai#</td>\n",
              "      <td>इसका# उलत# भी# सत्य# है#</td>\n",
              "      <td>@इसका उलट भी सत्य है#</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@kuch devta jo mukhyatah nagar devta the apne ...</td>\n",
              "      <td>कुछ# देवता# जो# मुख्यतः# नगर# देवता# दे# अपने#...</td>\n",
              "      <td>@कुछ देवता जो मुख्यत नगर देवता थे अपने संप्रदा...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@tel ke utpadan men sansar men romania ka chha...</td>\n",
              "      <td>तेल# के# उत्पादन# में# संसार# में# रोमानिया# क...</td>\n",
              "      <td>@तेल के उत्पादन में संसार में रोमानिया का छठा ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@banarasi lal se milkar police ne sara bhed pr...</td>\n",
              "      <td>बनारसी# लाल# से# मिलकर# पुलिस# ने# सारा# भेद# ...</td>\n",
              "      <td>@बनारसी लाल से मिलकर पुलिस ने सारा भेद प्राप्त...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>@jiske karan hrdapeshivikriti ya atalata ho sa...</td>\n",
              "      <td>जिसके# करण# हृदपेशीविकृतिकृतिकृतिकृतिकृतिकृत य...</td>\n",
              "      <td>@जिसके कारण हृदपेशीविकृति या अतालता हो सकती है#</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>@uttar pradesh vidhan sabha chunav men inhone ...</td>\n",
              "      <td>उत्तर# प्रदेश# विधान# सभा# चुनाव# में# इन्होंन...</td>\n",
              "      <td>@उत्तर प्रदेश विधान सभा चुनाव में इन्होंने उत्...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>@nepali upanyas ka shadharhu nepali bhasha ke ...</td>\n",
              "      <td>नैपाली# उपन्यास# का# शधारह# नैपाली# भाषा# के# ...</td>\n",
              "      <td>@नेपाली उपन्यास का आधारहरू नेपाली भाषा के विख्...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>@lekin maghi sahityetihaskaron ke anusar maghi...</td>\n",
              "      <td>लेकिन# मघी# साहित्येतिहासकारोंकारोंकारोंकारो क...</td>\n",
              "      <td>@लेकिन मगही साहित्येतिहासकारों के अनुसार मगही ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>@wiliam hover taft sanyukt rajya america ke ra...</td>\n",
              "      <td>विलियम्# हवर्# तफ्ट# संयुक्त# राज्य# अमेरिका# ...</td>\n",
              "      <td>@विलियम होवर टाफ्ट संयुक्त राज्य अमरीका के राष...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                input  ...                                           expected\n",
              "0   @kumbh rashi men janme log sambhavnaon se bhar...  ...  @कुंभ राशि में जन्मे लोग संभावनाओं से भरी एक ज...\n",
              "1                           @iska ulat bhi satya hai#  ...                              @इसका उलट भी सत्य है#\n",
              "2   @kuch devta jo mukhyatah nagar devta the apne ...  ...  @कुछ देवता जो मुख्यत नगर देवता थे अपने संप्रदा...\n",
              "3   @tel ke utpadan men sansar men romania ka chha...  ...  @तेल के उत्पादन में संसार में रोमानिया का छठा ...\n",
              "4   @banarasi lal se milkar police ne sara bhed pr...  ...  @बनारसी लाल से मिलकर पुलिस ने सारा भेद प्राप्त...\n",
              "..                                                ...  ...                                                ...\n",
              "95  @jiske karan hrdapeshivikriti ya atalata ho sa...  ...    @जिसके कारण हृदपेशीविकृति या अतालता हो सकती है#\n",
              "96  @uttar pradesh vidhan sabha chunav men inhone ...  ...  @उत्तर प्रदेश विधान सभा चुनाव में इन्होंने उत्...\n",
              "97  @nepali upanyas ka shadharhu nepali bhasha ke ...  ...  @नेपाली उपन्यास का आधारहरू नेपाली भाषा के विख्...\n",
              "98  @lekin maghi sahityetihaskaron ke anusar maghi...  ...  @लेकिन मगही साहित्येतिहासकारों के अनुसार मगही ...\n",
              "99  @wiliam hover taft sanyukt rajya america ke ra...  ...  @विलियम होवर टाफ्ट संयुक्त राज्य अमरीका के राष...\n",
              "\n",
              "[100 rows x 3 columns]"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res = pd.DataFrame(result, columns = ['input', 'output', 'expected'])\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "fVXdPsAvK3qD"
      },
      "outputs": [],
      "source": [
        "res.to_csv('/content/drive/MyDrive/NLP/file1.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ANLP Project (Attention, eng hin)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
